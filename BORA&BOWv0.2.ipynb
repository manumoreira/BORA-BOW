{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preparaci\u00f3n del entorno"
      ],
      "metadata": {
        "id": "CXOBm9wBGfa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "!pip install tabula-py\n",
        "!pip install pyPDF2\n",
        "# Import\n",
        "import tabula as tb\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import os\n",
        "import PyPDF2\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmyqpkxRGJJU",
        "outputId": "9ffaa346-743f-429d-e45c-1e21796a221d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V7dtDIyRD6EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Descarga de PDFs de BORA\n",
        "BORA genera PDFs de los boletines de cada d\u00eda.\n",
        "La primera secci\u00f3n es la que hace menci\u00f3n a Legislaci\u00f3n y avisos oficiales"
      ],
      "metadata": {
        "id": "xq2mPz5yGlIj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm3Qjzg1BpdU",
        "outputId": "aa8b9558-e75f-4fa1-add6-719618870444"
      },
      "outputs": [],
      "source": [
        "class BoletinDownloader:\n",
        "    def __init__(self, download_folder=\"boletin_pdfs\", cookie_file=\"cookies.txt\"):\n",
        "        self.session = requests.Session()\n",
        "        self.base_url = \"https://www.boletinoficial.gob.ar\"\n",
        "        self.download_url = f\"{self.base_url}/pdf/download_section\"\n",
        "        self.download_folder = download_folder\n",
        "        self.base_cookies = self._load_cookies_from_file(cookie_file)\n",
        "\n",
        "        os.makedirs(download_folder, exist_ok=True)\n",
        "\n",
        "        self.user_agent = (\n",
        "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
        "        )\n",
        "\n",
        "    def _load_cookies_from_file(self, cookie_file):\n",
        "        \"\"\"Load browser cookies (Netscape format) so the session keeps the date.\"\"\"\n",
        "        jar = requests.cookies.RequestsCookieJar()\n",
        "        if not cookie_file:\n",
        "            return jar\n",
        "        if not os.path.exists(cookie_file):\n",
        "            print(f\"Cookie file not found, continuing without it: {cookie_file}\")\n",
        "            return jar\n",
        "\n",
        "        with open(cookie_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            for raw_line in f:\n",
        "                line = raw_line.strip()\n",
        "                if not line or (line.startswith(\"#\") and not line.startswith(\"#HttpOnly_\")):\n",
        "                    continue\n",
        "                if line.startswith(\"#HttpOnly_\"):\n",
        "                    line = line.replace(\"#HttpOnly_\", \"\", 1)\n",
        "                parts = line.split(\"\t\")\n",
        "                if len(parts) != 7:\n",
        "                    continue\n",
        "                domain, flag, path, secure_flag, expiration, name, value = parts\n",
        "                cookie = requests.cookies.create_cookie(\n",
        "                    domain=domain,\n",
        "                    name=name,\n",
        "                    value=value,\n",
        "                    path=path,\n",
        "                    secure=secure_flag.upper() == \"TRUE\",\n",
        "                    expires=None if expiration == \"0\" else int(expiration)\n",
        "                )\n",
        "                jar.set_cookie(cookie)\n",
        "\n",
        "        return jar\n",
        "\n",
        "    def download_section(self, section, date):\n",
        "        \"\"\"Download a specific section for a given date (date is read from the session).\"\"\"\n",
        "        date_str = date.strftime(\"%Y%m%d\")\n",
        "        filename = f\"seccion_{section}_{date_str}.pdf\"\n",
        "        filepath = os.path.join(self.download_folder, filename)\n",
        "\n",
        "        if os.path.exists(filepath):\n",
        "            print(f\"\u2713 File already exists: {filename}\")\n",
        "            return True\n",
        "\n",
        "        # Reset cookies so each download starts from the exported browser session\n",
        "        self.session.cookies.clear()\n",
        "        if self.base_cookies:\n",
        "            self.session.cookies.update(self.base_cookies)\n",
        "\n",
        "        # First hit the date page so the backend stores the selected date in the session\n",
        "        section_url = f\"{self.base_url}/seccion/{section}/{date_str}\"\n",
        "        headers_get = {\n",
        "            'User-Agent': self.user_agent,\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate, br',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1'\n",
        "        }\n",
        "\n",
        "        date_page = self.session.get(\n",
        "            section_url,\n",
        "            headers=headers_get,\n",
        "            timeout=30,\n",
        "            allow_redirects=True\n",
        "        )\n",
        "\n",
        "        if date_page.status_code != 200:\n",
        "            print(f\"\u2717 Failed to access date page for {date_str}: HTTP {date_page.status_code}\")\n",
        "            return False\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "        headers_post = {\n",
        "            'User-Agent': self.user_agent,\n",
        "            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "            'X-Requested-With': 'XMLHttpRequest',\n",
        "            'Origin': self.base_url,\n",
        "            'Referer': section_url,\n",
        "            'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate, br',\n",
        "            'Connection': 'keep-alive'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = self.session.post(\n",
        "                self.download_url,\n",
        "                data={'nombreSeccion': section},\n",
        "                headers=headers_post,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                json_data = response.json()\n",
        "\n",
        "                if 'pdfBase64' in json_data and json_data['pdfBase64']:\n",
        "                    pdf_data = base64.b64decode(json_data['pdfBase64'])\n",
        "\n",
        "                    with open(filepath, 'wb') as f:\n",
        "                        f.write(pdf_data)\n",
        "\n",
        "                    print(f\"\u2713 Downloaded: {filename} ({len(pdf_data)} bytes)\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(f\"\u2717 No PDF available for {date_str}\")\n",
        "                    return False\n",
        "            else:\n",
        "                print(f\"\u2717 HTTP {response.status_code} for {date_str}\")\n",
        "                return False\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"\u2717 Network error for {date_str}: {e}\")\n",
        "            return False\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"\u2717 JSON decode error for {date_str}: {e}\")\n",
        "            print(f\"Raw response: {response.text[:200]}...\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"\u2717 Unexpected error for {date_str}: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "fechas_objetivo = [\n",
        "    '20250108', '20250115', '20250122', '20250205', '20250212', '20250226', '20250305', '20250312', '20250319', '20250402', '20250409', '20250416', '20250507', '20250514', '20250521', '20250604', '20250611', '20250618', '20250702', '20250709', '20250716', '20250806', '20250813', '20250820', '20250903', '20250910', '20250917', '20251008', '20251015', '20251105'\n",
        "]\n",
        "\n",
        "# Usa las cookies exportadas del navegador (cookies.txt) para que el backend reciba la fecha correcta\n",
        "# Tambi\u00e9n pod\u00e9s apuntar a cookies_20250108.txt si quer\u00e9s ese set espec\u00edfico.\n",
        "downloader = BoletinDownloader(cookie_file=\"cookies.txt\")\n",
        "\n",
        "print(f\"Iniciando descarga de {len(fechas_objetivo)} fechas...\")\n",
        "\n",
        "for fecha_str in fechas_objetivo:\n",
        "    fecha_dt = datetime.strptime(fecha_str, \"%Y%m%d\")\n",
        "    downloader.download_section(\"primera\", fecha_dt)\n",
        "\n",
        "print(\"\u2713 Proceso de descarga finalizado.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Implementando Bag of words\n",
        "Bag of words es un modelo muy utilizado en text mining.\n",
        "Tiene la particularidad de adaptarse bien a corpus textuales muy diversos permitiendo describir el corpus sin ser especialista en el dominio"
      ],
      "metadata": {
        "id": "uCOtpYDlHSQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Preparar dataset e importarinstalar librerias"
      ],
      "metadata": {
        "id": "onLZ9lYl-3mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import and install\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "spanish_stopwords = stopwords.words('spanish')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import spacy\n",
        "import locale\n",
        "import string\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "! spacy download es_core_news_sm\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "stop_words_adicionales = [\n",
        "    'art\u00edculo', 'art', 'ley', 'nacional', 'decreto',\n",
        "    'resoluci\u00f3n', 'disposici\u00f3n', 'expediente', 'anexo',\n",
        "    'p\u00fablico', 'oficial', 'ministerio', 'bolet\u00edn',\n",
        "    'direcci\u00f3n', 'secretar\u00eda', 'presente', 'fecha',\n",
        "    'buenos', 'aires', 'conforme', 'firma', 'administrativo',\n",
        "    'conforme', 'establecido', 'medida', 'registro', 'regulaci\u00f3n',\n",
        "    'modificatoria', 'secci\u00f3n', 'dispuesto', 'dictar'\n",
        "]\n",
        "\n",
        "# Actualizamos el modelo de spaCy para que marque estas palabras como stop words\n",
        "for palabra in stop_words_adicionales:\n",
        "    # Agregamos la palabra tal cual\n",
        "    nlp.vocab[palabra].is_stop = True\n",
        "    # Tambi\u00e9n es \u00fatil agregar la versi\u00f3n en min\u00fascula por las dudas\n",
        "    nlp.vocab[palabra.lower()].is_stop = True\n"
      ],
      "metadata": {
        "id": "ob7G_Q0sHmlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93c319d1-0b98-4291-820a-b449c40da92b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe organized in paragraphs\n",
        "def extract_paragraphs_to_dataframe(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text at paragraph level for better context\n",
        "    \"\"\"\n",
        "    paragraphs_data = []\n",
        "\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "\n",
        "            # Split into paragraphs\n",
        "            page_paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "            for para_num, paragraph in enumerate(page_paragraphs):\n",
        "                paragraphs_data.append({\n",
        "                    'page': page_num + 1,\n",
        "                    'paragraph_id': f\"p{page_num+1}_{para_num+1}\",\n",
        "                    'text': paragraph,\n",
        "                    'char_count': len(paragraph),\n",
        "                    'word_count': len(paragraph.split())\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(paragraphs_data)\n",
        "\n",
        "lista_dfs = []\n",
        "carpeta_pdf = \"boletin_pdfs\"\n",
        "\n",
        "print(\"Comenzando extracci\u00f3n de texto...\")\n",
        "\n",
        "for fecha_str in fechas_objetivo:\n",
        "    # Reconstruimos el nombre del archivo seg\u00fan la l\u00f3gica del downloader\n",
        "    nombre_archivo = f\"seccion_primera_{fecha_str}.pdf\"\n",
        "    path_archivo = os.path.join(carpeta_pdf, nombre_archivo)\n",
        "\n",
        "    if os.path.exists(path_archivo):\n",
        "        try:\n",
        "            # Extraemos data\n",
        "            df_temp = extract_paragraphs_to_dataframe(path_archivo)\n",
        "\n",
        "            # Agregamos columna de fecha para trazabilidad (\u00fatil si luego cambias de opini\u00f3n sobre agrupar)\n",
        "            df_temp['fecha_boletin'] = fecha_str\n",
        "\n",
        "            lista_dfs.append(df_temp)\n",
        "            print(f\"\u2713 Procesado: {nombre_archivo} ({len(df_temp)} p\u00e1rrafos)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\u26a0 Error leyendo {nombre_archivo}: {e}\")\n",
        "    else:\n",
        "        # Esto pasa si era fin de semana o feriado y el archivo no se baj\u00f3\n",
        "        pass\n",
        "\n",
        "# Unimos todo en una sola bolsa\n",
        "if lista_dfs:\n",
        "    df_consolidado = pd.concat(lista_dfs, ignore_index=True)\n",
        "    print(f\"\\nTOTAL: {len(df_consolidado)} p\u00e1rrafos extra\u00eddos de {len(lista_dfs)} documentos.\")\n",
        "    print(df_consolidado.head())\n",
        "else:\n",
        "    print(\"Error: No se encontraron PDFs para procesar.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgLAwDugv6FH",
        "outputId": "7292d8a3-ec29-4985-c059-2780820cd3ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Funciones para BOW"
      ],
      "metadata": {
        "id": "s3kFrcVK-uTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_spanish_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess a single Spanish text with lemmatization\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text == '':\n",
        "        return \"\"\n",
        "\n",
        "    doc = nlp(text.lower())\n",
        "\n",
        "    lemmas = []\n",
        "    for token in doc:\n",
        "        if (not token.is_stop and\n",
        "            not token.is_punct and\n",
        "            not token.is_space and\n",
        "            len(token.text) > 2 and\n",
        "            token.is_alpha):\n",
        "            lemmas.append(token.lemma_)\n",
        "\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "def apply_text_preprocessing(df, text_column):\n",
        "    \"\"\"\n",
        "    Apply preprocessing to a DataFrame column\n",
        "    Returns a new DataFrame with processed text\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    df_processed['processed_text'] = df_processed[text_column].apply(preprocess_spanish_text)\n",
        "    df_processed['original_word_count'] = df_processed[text_column].str.split().str.len()\n",
        "    df_processed['processed_word_count'] = df_processed['processed_text'].str.split().str.len()\n",
        "\n",
        "    return df_processed\n",
        "\n",
        "def calculate_text_statistics(df, original_col, processed_col):\n",
        "    \"\"\"\n",
        "    Calculate text statistics for comparison\n",
        "    \"\"\"\n",
        "    total_original = df[original_col].str.split().str.len().sum()\n",
        "    total_processed = df[processed_col].str.split().str.len().sum()\n",
        "\n",
        "    all_original = ' '.join(df[original_col].astype(str))\n",
        "    all_processed = ' '.join(df[processed_col].astype(str))\n",
        "\n",
        "    unique_original = len(set(all_original.lower().split()))\n",
        "    unique_processed = len(set(all_processed.split()))\n",
        "\n",
        "    stats = {\n",
        "        'total_paragraphs': len(df),\n",
        "        'total_original_words': total_original,\n",
        "        'total_processed_words': total_processed,\n",
        "        'unique_original_words': unique_original,\n",
        "        'unique_processed_words': unique_processed,\n",
        "        'word_reduction_pct': ((total_original - total_processed) / total_original * 100),\n",
        "        'vocab_reduction_pct': ((unique_original - unique_processed) / unique_original * 100)\n",
        "    }\n",
        "\n",
        "    return stats\n",
        "\n",
        "def create_bow_analysis(text_series, top_n=20, **vectorizer_kwargs):\n",
        "    \"\"\"\n",
        "    Create Bag-of-Words analysis from a text series\n",
        "    \"\"\"\n",
        "    # Combine all text\n",
        "    all_text = ' '.join(text_series.astype(str))\n",
        "\n",
        "    # Default vectorizer parameters\n",
        "    default_params = {\n",
        "        'max_features': top_n * 2,\n",
        "        'lowercase': True,\n",
        "        'token_pattern': r'\\b[a-zA-Z\u00e1\u00e9\u00ed\u00f3\u00fa\u00f1\u00c1\u00c9\u00cd\u00d3\u00da\u00d1]+\\b'\n",
        "    }\n",
        "    default_params.update(vectorizer_kwargs)\n",
        "\n",
        "    vectorizer = CountVectorizer(**default_params)\n",
        "    bow_matrix = vectorizer.fit_transform([all_text])\n",
        "\n",
        "    # Get word frequencies\n",
        "    word_freq = dict(zip(vectorizer.get_feature_names_out(), bow_matrix.sum(axis=0).tolist()[0]))\n",
        "    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "    return pd.DataFrame(top_words, columns=['word', 'frequency'])\n",
        "\n",
        "\n",
        "def print_preprocessing_report(df, original_col='text', processed_col='processed_text'):\n",
        "    \"\"\"\n",
        "    Print a comprehensive preprocessing report\n",
        "    \"\"\"\n",
        "    stats = calculate_text_statistics(df, original_col, processed_col)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"TEXT PREPROCESSING REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"Total paragraphs: {stats['total_paragraphs']:,}\")\n",
        "    print(f\"Total words - Original: {stats['total_original_words']:,}\")\n",
        "    print(f\"Total words - Processed: {stats['total_processed_words']:,}\")\n",
        "    print(f\"Word reduction: {stats['word_reduction_pct']:.1f}%\")\n",
        "    print(f\"Unique words - Original: {stats['unique_original_words']:,}\")\n",
        "    print(f\"Unique words - Processed: {stats['unique_processed_words']:,}\")\n",
        "    print(f\"Vocabulary reduction: {stats['vocab_reduction_pct']:.1f}%\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "def print_sample_transformations(df, original_col='text', processed_col='processed_text', sample_size=3):\n",
        "    \"\"\"\n",
        "    Print sample transformations for quality check\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SAMPLE TRANSFORMATIONS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for i in range(min(sample_size, len(df))):\n",
        "        original = df.iloc[i][original_col]\n",
        "        processed = df.iloc[i][processed_col]\n",
        "\n",
        "        # Try to get page number if available\n",
        "        page_info = f\"Page {df.iloc[i]['page']}\" if 'page' in df.columns else f\"Row {i+1}\"\n",
        "\n",
        "        print(f\"\\nSample {i+1} ({page_info}):\")\n",
        "        print(f\"ORIGINAL: {original[:150]}...\")\n",
        "        print(f\"PROCESSED: {processed[:150]}...\")\n",
        "\n",
        "        if 'original_word_count' in df.columns and 'processed_word_count' in df.columns:\n",
        "            print(f\"Words: {df.iloc[i]['original_word_count']} \u2192 {df.iloc[i]['processed_word_count']}\")\n",
        "\n",
        "\n",
        "def process_paragraphs_dataframe(df_paragraphs, text_column='text'):\n",
        "    \"\"\"\n",
        "    Specific function to process your paragraphs DataFrame\n",
        "    \"\"\"\n",
        "    print(\"Applying lemmatization to paragraphs...\")\n",
        "    df_processed = apply_text_preprocessing(df_paragraphs, text_column)\n",
        "\n",
        "    # Filter out empty processed paragraphs\n",
        "    df_processed = df_processed[df_processed['processed_text'].str.len() > 0].copy()\n",
        "\n",
        "    print(f\"Original paragraphs: {len(df_paragraphs)}\")\n",
        "    print(f\"Paragraphs after processing: {len(df_processed)}\")\n",
        "\n",
        "    return df_processed\n",
        "\n",
        "def analyze_paragraphs_bow(df_processed, text_column='processed_text', top_n=20):\n",
        "    \"\"\"\n",
        "    Specific function to analyze preprocessed paragraphs with BOW\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BAG OF WORDS ANALYSIS - PROCESSED PARAGRAPHS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    top_words_df = create_bow_analysis(df_processed[text_column], top_n=top_n)\n",
        "\n",
        "    print(f\"Top {len(top_words_df)} most frequent lemmas:\")\n",
        "    print(top_words_df.to_string(index=False))\n",
        "\n",
        "    return top_words_df\n",
        "\n",
        "\n",
        "def run_paragraphs_analysis_pipeline(df_paragraphs, text_column='text', top_n=20):\n",
        "    \"\"\"\n",
        "    Complete pipeline for analyzing your paragraphs data\n",
        "    \"\"\"\n",
        "    # Step 1: Preprocess data\n",
        "    df_processed = process_paragraphs_dataframe(df_paragraphs, text_column)\n",
        "\n",
        "    # Step 2: Generate reports\n",
        "    stats = print_preprocessing_report(df_processed)\n",
        "    print_sample_transformations(df_processed)\n",
        "\n",
        "    # Step 3: BOW analysis\n",
        "    top_words = analyze_paragraphs_bow(df_processed, top_n=top_n)\n",
        "\n",
        "    return df_processed, top_words, stats\n",
        "\n",
        "# =============================================================================\n",
        "# USO\n",
        "# =============================================================================\n",
        "\n",
        "# Complete pipeline (your main use case)\n",
        "if 'df_consolidado' in locals() and not df_consolidado.empty:\n",
        "    print(f\"Analizando corpus de {df_consolidado['fecha_boletin'].nunique()} fechas...\\n\")\n",
        "\n",
        "    df_processed, top_words, stats = run_paragraphs_analysis_pipeline(df_consolidado)\n",
        "\n",
        "    # Opcional: Ver si el top 20 cambia mucho al tener m\u00e1s fechas\n",
        "    print(\"\\nTop palabras del periodo completo:\")\n",
        "    print(top_words)\n",
        "else:\n",
        "    print(\"No hay datos consolidados para analizar.\")\n",
        "\n",
        "# Just BOW analysis on already processed data\n",
        "#top_words = analyze_paragraphs_bow(df_processed)\n",
        "\n",
        "# Compare original vs processed BOW\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON: ORIGINAL vs PROCESSED BOW\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "bow_original = create_bow_analysis(df_processed['text'], top_n=30)\n",
        "bow_processed = create_bow_analysis(df_processed['processed_text'], top_n=30)\n",
        "\n",
        "print(\"Top 30 words - ORIGINAL:\")\n",
        "print(bow_original.to_string(index=False))\n",
        "\n",
        "print(\"\\nTop 30 words - PROCESSED (lemmatized):\")\n",
        "print(bow_processed.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5iwoqYToOEB",
        "outputId": "a3843fca-8a33-4833-e510-e85f27f1954a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 1. Generamos el dataframe con TODAS las palabras (o un n\u00famero muy alto, ej: 10.000)\n",
        "# Usamos tu funci\u00f3n existente 'create_bow_analysis' pero pedimos m\u00e1s palabras\n",
        "print(\"Generando reporte completo...\")\n",
        "df_frecuencias_full = create_bow_analysis(df_processed['processed_text'], top_n=10000)\n",
        "\n",
        "# 2. Definimos el nombre del archivo\n",
        "nombre_archivo = \"frecuencias_palabras_bora.csv\"\n",
        "\n",
        "# 3. Guardamos en CSV\n",
        "# Usamos encoding='utf-8-sig' para que Excel reconozca bien los acentos (\u00f1, \u00e1, \u00e9...)\n",
        "df_frecuencias_full.to_csv(nombre_archivo, index=False, encoding='utf-8-sig')\n",
        "\n",
        "# 4. Descargamos el archivo a tu PC\n",
        "files.download(nombre_archivo)\n",
        "\n",
        "print(f\"\u2713 Archivo '{nombre_archivo}' generado y descargado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ChLHsE31to5K",
        "outputId": "7fb1774e-4be7-4d14-9933-cb29eca99b54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Otros\n",
        "\n",
        "Algunos scripts para explorar el contenido de los PDFs"
      ],
      "metadata": {
        "id": "P5QNMWw7HOE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tabula\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "import requests\n",
        "\n",
        "# Since we're in Colab, let's work with the downloaded PDF file\n",
        "pdf_path = \"boletin_pdfs/seccion_primera_20250927.pdf\"\n",
        "\n",
        "# First, let's get some basic information about the PDF\n",
        "# Get the number of pages\n",
        "import PyPDF2\n",
        "with open(pdf_path, 'rb') as file:\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    num_pages = len(pdf_reader.pages)\n",
        "    print(f\"Number of pages in the PDF: {num_pages}\")\n",
        "\n",
        "# Now let's extract all the text to search for \"derechos humanos\"\n",
        "full_text = \"\"\n",
        "for page_num in range(num_pages):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        full_text += page.extract_text()\n",
        "\n",
        "# Count occurrences of \"derechos humanos\"\n",
        "derechos_humanos_count = full_text.lower().count(\"derechos humanos\")\n",
        "print(f\"Occurrences of 'derechos humanos': {derechos_humanos_count}\")\n",
        "\n",
        "# Now let's use tabula to extract tables and look for resoluciones and disposiciones\n",
        "# Let's try to extract all tables from the PDF\n",
        "tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n",
        "print(f\"Number of tables found: {len(tables)}\")\n",
        "\n",
        "# Let's examine the structure of the first few tables to understand the content\n",
        "for i, table in enumerate(tables[:5]):\n",
        "    print(f\"\\nTable {i+1} shape: {table.shape}\")\n",
        "    print(f\"Table {i+1} columns: {table.columns.tolist()}\")\n",
        "    if not table.empty:\n",
        "        print(table.head(3))"
      ],
      "metadata": {
        "id": "LBnXb4Q8F8Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listar resoluciones\n",
        "\n",
        "# Extract text from page 2 specifically\n",
        "with open(pdf_path, 'rb') as file:\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    page2_text = pdf_reader.pages[1].extract_text()  # Page index 1 is page 2\n",
        "\n",
        "print(\"Content from page 2 (SUMARIO):\")\n",
        "print(page2_text[:2000])  # First 2000 characters\n",
        "\n",
        "# Let's search for resolution patterns in page 2 specifically\n",
        "resoluciones_page2 = re.findall(resoluciones_pattern, page2_text, re.IGNORECASE)\n",
        "print(f\"\\nResoluciones found in SUMARIO page: {len(set(resoluciones_page2))}\")\n",
        "for resol in sorted(set(resoluciones_page2)):\n",
        "    print(f\"  - {resol}\")\n",
        "\n",
        "# Let's also check if we can find the specific sections for Resoluciones and Disposiciones\n",
        "# by looking at the table of contents structure"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hndEDCh4G72j",
        "outputId": "bda3ea44-d13e-43ee-c972-7637f39938f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe de prueba\n",
        "# Cambiar el pdf path al que se haya descargado en \ud83d\udcc1\n",
        "pdf_path = \"boletin_pdfs/seccion_primera_20251020.pdf\"\n",
        "with open(pdf_path, 'rb') as file:\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    num_pages = len(pdf_reader.pages)\n",
        "    print(f\"Number of pages in the PDF: {num_pages}\")\n",
        "\n",
        "full_text = \"\"\n",
        "for page_num in range(num_pages):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        full_text += page.extract_text()\n",
        "df_test =  full_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mqam4ivskxt",
        "outputId": "7e021d23-c5b0-46ff-d834-26782c46bbdc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}