{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preparación del entorno"
      ],
      "metadata": {
        "id": "CXOBm9wBGfa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "!pip install tabula-py\n",
        "!pip install pyPDF2\n",
        "# Import\n",
        "import tabula as tb\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import os\n",
        "import PyPDF2\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmyqpkxRGJJU",
        "outputId": "9ffaa346-743f-429d-e45c-1e21796a221d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.12/dist-packages (from tabula-py) (2.2.2)\n",
            "Requirement already satisfied: numpy>1.24.4 in /usr/local/lib/python3.12/dist-packages (from tabula-py) (2.0.2)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.12/dist-packages (from tabula-py) (1.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.17.0)\n",
            "Downloading tabula_py-2.10.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tabula-py\n",
            "Successfully installed tabula-py-2.10.0\n",
            "Collecting pyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyPDF2\n",
            "Successfully installed pyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V7dtDIyRD6EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Descarga de PDFs de BORA\n",
        "BORA genera PDFs de los boletines de cada día.\n",
        "La primera sección es la que hace mención a Legislación y avisos oficiales"
      ],
      "metadata": {
        "id": "xq2mPz5yGlIj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm3Qjzg1BpdU",
        "outputId": "aa8b9558-e75f-4fa1-add6-719618870444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando descarga de 30 fechas...\n",
            "\n",
            "✓ Downloaded: seccion_primera_20250108.pdf (2493313 bytes)\n",
            "✓ Downloaded: seccion_primera_20250115.pdf (2493313 bytes)\n",
            "✗ Network error for 20250122: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓ Downloaded: seccion_primera_20250205.pdf (2493313 bytes)\n",
            "✗ Network error for 20250212: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓ Downloaded: seccion_primera_20250226.pdf (2493313 bytes)\n",
            "✗ Network error for 20250305: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓ Downloaded: seccion_primera_20250312.pdf (2493313 bytes)\n",
            "✓ Downloaded: seccion_primera_20250319.pdf (2493313 bytes)\n",
            "✗ Network error for 20250402: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓ Downloaded: seccion_primera_20250409.pdf (2493313 bytes)\n",
            "✓ Downloaded: seccion_primera_20250416.pdf (2493313 bytes)\n",
            "✓ Downloaded: seccion_primera_20250507.pdf (2493313 bytes)\n",
            "✓ Downloaded: seccion_primera_20250514.pdf (2493313 bytes)\n",
            "✗ Network error for 20250521: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓ Downloaded: seccion_primera_20250604.pdf (2493313 bytes)\n",
            "✓ Downloaded: seccion_primera_20250611.pdf (2493313 bytes)\n",
            "✓ Downloaded: seccion_primera_20250618.pdf (2493313 bytes)\n",
            "✓ Downloaded: seccion_primera_20250702.pdf (2493313 bytes)\n",
            "✓ Downloaded: seccion_primera_20250709.pdf (2493313 bytes)\n",
            "✓ Downloaded: seccion_primera_20250716.pdf (2493313 bytes)\n",
            "✗ Network error for 20250806: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓ Downloaded: seccion_primera_20250813.pdf (2493313 bytes)\n",
            "✓ Downloaded: seccion_primera_20250820.pdf (2493313 bytes)\n",
            "✗ Network error for 20250903: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓ Downloaded: seccion_primera_20250910.pdf (2493313 bytes)\n",
            "✗ Network error for 20250917: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓ Downloaded: seccion_primera_20251008.pdf (2493313 bytes)\n",
            "✓ Downloaded: seccion_primera_20251015.pdf (2493313 bytes)\n",
            "✓ Downloaded: seccion_primera_20251105.pdf (2493313 bytes)\n",
            "\n",
            "✓ Proceso de descarga finalizado.\n"
          ]
        }
      ],
      "source": [
        "class BoletinDownloader:\n",
        "    def __init__(self, download_folder=\"boletin_pdfs\"):\n",
        "        self.session = requests.Session()\n",
        "        self.base_url = \"https://www.boletinoficial.gob.ar\"\n",
        "        self.download_url = f\"{self.base_url}/pdf/download_section\"\n",
        "        self.download_folder = download_folder\n",
        "\n",
        "        # Create download folder if it doesn't exist\n",
        "        os.makedirs(download_folder, exist_ok=True)\n",
        "\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "            'X-Requested-With': 'XMLHttpRequest',\n",
        "            'Origin': self.base_url,\n",
        "            'Referer': f'{self.base_url}/'\n",
        "        }\n",
        "\n",
        "    def download_section(self, section, date):\n",
        "        \"\"\"Download a specific section for a given date\"\"\"\n",
        "        date_str = date.strftime(\"%Y%m%d\")\n",
        "        filename = f\"seccion_{section}_{date_str}.pdf\"\n",
        "        filepath = os.path.join(self.download_folder, filename)\n",
        "\n",
        "        # Check if file already exists\n",
        "        if os.path.exists(filepath):\n",
        "            print(f\"✓ File already exists: {filename}\")\n",
        "            return True\n",
        "\n",
        "        data = {'nombreSeccion': section}\n",
        "\n",
        "        try:\n",
        "            response = self.session.post(\n",
        "                self.download_url,\n",
        "                data=data,\n",
        "                headers=self.headers,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                json_data = response.json()\n",
        "\n",
        "                if 'pdfBase64' in json_data and json_data['pdfBase64']:\n",
        "                    pdf_data = base64.b64decode(json_data['pdfBase64'])\n",
        "\n",
        "                    with open(filepath, 'wb') as f:\n",
        "                        f.write(pdf_data)\n",
        "\n",
        "                    print(f\"✓ Downloaded: {filename} ({len(pdf_data)} bytes)\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(f\"✗ No PDF available for {date_str}\")\n",
        "                    return False\n",
        "            else:\n",
        "                print(f\"✗ HTTP {response.status_code} for {date_str}\")\n",
        "                return False\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"✗ Network error for {date_str}: {e}\")\n",
        "            return False\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"✗ JSON decode error for {date_str}: {e}\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Unexpected error for {date_str}: {e}\")\n",
        "            return False\n",
        "\n",
        "fechas_objetivo = [\n",
        "    '20250108', '20250115', '20250122', '20250205', '20250212', '20250226', '20250305', '20250312', '20250319', '20250402', '20250409', '20250416', '20250507', '20250514', '20250521', '20250604', '20250611', '20250618', '20250702', '20250709', '20250716', '20250806', '20250813', '20250820', '20250903', '20250910', '20250917', '20251008', '20251015', '20251105'\n",
        "]\n",
        "\n",
        "downloader = BoletinDownloader()\n",
        "\n",
        "print(f\"Iniciando descarga de {len(fechas_objetivo)} fechas...\\n\")\n",
        "\n",
        "for fecha_str in fechas_objetivo:\n",
        "    # Convertimos string a datetime\n",
        "    fecha_dt = datetime.strptime(fecha_str, \"%Y%m%d\")\n",
        "\n",
        "    # La clase imprimirá si baja el archivo o si no existe\n",
        "    downloader.download_section(\"primera\", fecha_dt)\n",
        "\n",
        "print(\"\\n✓ Proceso de descarga finalizado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Implementando Bag of words\n",
        "Bag of words es un modelo muy utilizado en text mining.\n",
        "Tiene la particularidad de adaptarse bien a corpus textuales muy diversos permitiendo describir el corpus sin ser especialista en el dominio"
      ],
      "metadata": {
        "id": "uCOtpYDlHSQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Preparar dataset e importarinstalar librerias"
      ],
      "metadata": {
        "id": "onLZ9lYl-3mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import and install\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "spanish_stopwords = stopwords.words('spanish')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import spacy\n",
        "import locale\n",
        "import string\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "! spacy download es_core_news_sm\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "stop_words_adicionales = [\n",
        "    'artículo', 'art', 'ley', 'nacional', 'decreto',\n",
        "    'resolución', 'disposición', 'expediente', 'anexo',\n",
        "    'público', 'oficial', 'ministerio', 'boletín',\n",
        "    'dirección', 'secretaría', 'presente', 'fecha',\n",
        "    'buenos', 'aires', 'conforme', 'firma', 'administrativo',\n",
        "    'conforme', 'establecido', 'medida', 'registro', 'regulación',\n",
        "    'modificatoria', 'sección', 'dispuesto', 'dictar'\n",
        "]\n",
        "\n",
        "# Actualizamos el modelo de spaCy para que marque estas palabras como stop words\n",
        "for palabra in stop_words_adicionales:\n",
        "    # Agregamos la palabra tal cual\n",
        "    nlp.vocab[palabra].is_stop = True\n",
        "    # También es útil agregar la versión en minúscula por las dudas\n",
        "    nlp.vocab[palabra.lower()].is_stop = True\n"
      ],
      "metadata": {
        "id": "ob7G_Q0sHmlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93c319d1-0b98-4291-820a-b449c40da92b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe organized in paragraphs\n",
        "def extract_paragraphs_to_dataframe(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text at paragraph level for better context\n",
        "    \"\"\"\n",
        "    paragraphs_data = []\n",
        "\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "\n",
        "            # Split into paragraphs\n",
        "            page_paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "            for para_num, paragraph in enumerate(page_paragraphs):\n",
        "                paragraphs_data.append({\n",
        "                    'page': page_num + 1,\n",
        "                    'paragraph_id': f\"p{page_num+1}_{para_num+1}\",\n",
        "                    'text': paragraph,\n",
        "                    'char_count': len(paragraph),\n",
        "                    'word_count': len(paragraph.split())\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(paragraphs_data)\n",
        "\n",
        "lista_dfs = []\n",
        "carpeta_pdf = \"boletin_pdfs\"\n",
        "\n",
        "print(\"Comenzando extracción de texto...\")\n",
        "\n",
        "for fecha_str in fechas_objetivo:\n",
        "    # Reconstruimos el nombre del archivo según la lógica del downloader\n",
        "    nombre_archivo = f\"seccion_primera_{fecha_str}.pdf\"\n",
        "    path_archivo = os.path.join(carpeta_pdf, nombre_archivo)\n",
        "\n",
        "    if os.path.exists(path_archivo):\n",
        "        try:\n",
        "            # Extraemos data\n",
        "            df_temp = extract_paragraphs_to_dataframe(path_archivo)\n",
        "\n",
        "            # Agregamos columna de fecha para trazabilidad (útil si luego cambias de opinión sobre agrupar)\n",
        "            df_temp['fecha_boletin'] = fecha_str\n",
        "\n",
        "            lista_dfs.append(df_temp)\n",
        "            print(f\"✓ Procesado: {nombre_archivo} ({len(df_temp)} párrafos)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error leyendo {nombre_archivo}: {e}\")\n",
        "    else:\n",
        "        # Esto pasa si era fin de semana o feriado y el archivo no se bajó\n",
        "        pass\n",
        "\n",
        "# Unimos todo en una sola bolsa\n",
        "if lista_dfs:\n",
        "    df_consolidado = pd.concat(lista_dfs, ignore_index=True)\n",
        "    print(f\"\\nTOTAL: {len(df_consolidado)} párrafos extraídos de {len(lista_dfs)} documentos.\")\n",
        "    print(df_consolidado.head())\n",
        "else:\n",
        "    print(\"Error: No se encontraron PDFs para procesar.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgLAwDugv6FH",
        "outputId": "7292d8a3-ec29-4985-c059-2780820cd3ef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comenzando extracción de texto...\n",
            "✓ Procesado: seccion_primera_20250108.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250115.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250205.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250226.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250312.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250319.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250409.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250416.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250507.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250514.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250604.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250611.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250618.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250702.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250709.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250716.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250813.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250820.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20250910.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20251008.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20251015.pdf (110 párrafos)\n",
            "✓ Procesado: seccion_primera_20251105.pdf (110 párrafos)\n",
            "\n",
            "TOTAL: 2420 párrafos extraídos de 22 documentos.\n",
            "   page paragraph_id                                               text  \\\n",
            "0     1         p1_1  Registro Nacional de la Propiedad Intelectual ...   \n",
            "1     2         p2_1  BOLETÍN OFICIAL Nº 35.809 - Primera Sección 2 ...   \n",
            "2     3         p3_1  BOLETÍN OFICIAL Nº 35.809 - Primera Sección 3 ...   \n",
            "3     4         p4_1  BOLETÍN OFICIAL Nº 35.809 - Primera Sección 4 ...   \n",
            "4     5         p5_1  BOLETÍN OFICIAL Nº 35.809 - Primera Sección 5 ...   \n",
            "\n",
            "   char_count  word_count fecha_boletin  \n",
            "0         830         117      20250108  \n",
            "1        5910         396      20250108  \n",
            "2        3950         201      20250108  \n",
            "3        4150         623      20250108  \n",
            "4        3436         540      20250108  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Funciones para BOW"
      ],
      "metadata": {
        "id": "s3kFrcVK-uTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_spanish_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess a single Spanish text with lemmatization\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text == '':\n",
        "        return \"\"\n",
        "\n",
        "    doc = nlp(text.lower())\n",
        "\n",
        "    lemmas = []\n",
        "    for token in doc:\n",
        "        if (not token.is_stop and\n",
        "            not token.is_punct and\n",
        "            not token.is_space and\n",
        "            len(token.text) > 2 and\n",
        "            token.is_alpha):\n",
        "            lemmas.append(token.lemma_)\n",
        "\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "def apply_text_preprocessing(df, text_column):\n",
        "    \"\"\"\n",
        "    Apply preprocessing to a DataFrame column\n",
        "    Returns a new DataFrame with processed text\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    df_processed['processed_text'] = df_processed[text_column].apply(preprocess_spanish_text)\n",
        "    df_processed['original_word_count'] = df_processed[text_column].str.split().str.len()\n",
        "    df_processed['processed_word_count'] = df_processed['processed_text'].str.split().str.len()\n",
        "\n",
        "    return df_processed\n",
        "\n",
        "def calculate_text_statistics(df, original_col, processed_col):\n",
        "    \"\"\"\n",
        "    Calculate text statistics for comparison\n",
        "    \"\"\"\n",
        "    total_original = df[original_col].str.split().str.len().sum()\n",
        "    total_processed = df[processed_col].str.split().str.len().sum()\n",
        "\n",
        "    all_original = ' '.join(df[original_col].astype(str))\n",
        "    all_processed = ' '.join(df[processed_col].astype(str))\n",
        "\n",
        "    unique_original = len(set(all_original.lower().split()))\n",
        "    unique_processed = len(set(all_processed.split()))\n",
        "\n",
        "    stats = {\n",
        "        'total_paragraphs': len(df),\n",
        "        'total_original_words': total_original,\n",
        "        'total_processed_words': total_processed,\n",
        "        'unique_original_words': unique_original,\n",
        "        'unique_processed_words': unique_processed,\n",
        "        'word_reduction_pct': ((total_original - total_processed) / total_original * 100),\n",
        "        'vocab_reduction_pct': ((unique_original - unique_processed) / unique_original * 100)\n",
        "    }\n",
        "\n",
        "    return stats\n",
        "\n",
        "def create_bow_analysis(text_series, top_n=20, **vectorizer_kwargs):\n",
        "    \"\"\"\n",
        "    Create Bag-of-Words analysis from a text series\n",
        "    \"\"\"\n",
        "    # Combine all text\n",
        "    all_text = ' '.join(text_series.astype(str))\n",
        "\n",
        "    # Default vectorizer parameters\n",
        "    default_params = {\n",
        "        'max_features': top_n * 2,\n",
        "        'lowercase': True,\n",
        "        'token_pattern': r'\\b[a-zA-ZáéíóúñÁÉÍÓÚÑ]+\\b'\n",
        "    }\n",
        "    default_params.update(vectorizer_kwargs)\n",
        "\n",
        "    vectorizer = CountVectorizer(**default_params)\n",
        "    bow_matrix = vectorizer.fit_transform([all_text])\n",
        "\n",
        "    # Get word frequencies\n",
        "    word_freq = dict(zip(vectorizer.get_feature_names_out(), bow_matrix.sum(axis=0).tolist()[0]))\n",
        "    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "    return pd.DataFrame(top_words, columns=['word', 'frequency'])\n",
        "\n",
        "\n",
        "def print_preprocessing_report(df, original_col='text', processed_col='processed_text'):\n",
        "    \"\"\"\n",
        "    Print a comprehensive preprocessing report\n",
        "    \"\"\"\n",
        "    stats = calculate_text_statistics(df, original_col, processed_col)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"TEXT PREPROCESSING REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"Total paragraphs: {stats['total_paragraphs']:,}\")\n",
        "    print(f\"Total words - Original: {stats['total_original_words']:,}\")\n",
        "    print(f\"Total words - Processed: {stats['total_processed_words']:,}\")\n",
        "    print(f\"Word reduction: {stats['word_reduction_pct']:.1f}%\")\n",
        "    print(f\"Unique words - Original: {stats['unique_original_words']:,}\")\n",
        "    print(f\"Unique words - Processed: {stats['unique_processed_words']:,}\")\n",
        "    print(f\"Vocabulary reduction: {stats['vocab_reduction_pct']:.1f}%\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "def print_sample_transformations(df, original_col='text', processed_col='processed_text', sample_size=3):\n",
        "    \"\"\"\n",
        "    Print sample transformations for quality check\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SAMPLE TRANSFORMATIONS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for i in range(min(sample_size, len(df))):\n",
        "        original = df.iloc[i][original_col]\n",
        "        processed = df.iloc[i][processed_col]\n",
        "\n",
        "        # Try to get page number if available\n",
        "        page_info = f\"Page {df.iloc[i]['page']}\" if 'page' in df.columns else f\"Row {i+1}\"\n",
        "\n",
        "        print(f\"\\nSample {i+1} ({page_info}):\")\n",
        "        print(f\"ORIGINAL: {original[:150]}...\")\n",
        "        print(f\"PROCESSED: {processed[:150]}...\")\n",
        "\n",
        "        if 'original_word_count' in df.columns and 'processed_word_count' in df.columns:\n",
        "            print(f\"Words: {df.iloc[i]['original_word_count']} → {df.iloc[i]['processed_word_count']}\")\n",
        "\n",
        "\n",
        "def process_paragraphs_dataframe(df_paragraphs, text_column='text'):\n",
        "    \"\"\"\n",
        "    Specific function to process your paragraphs DataFrame\n",
        "    \"\"\"\n",
        "    print(\"Applying lemmatization to paragraphs...\")\n",
        "    df_processed = apply_text_preprocessing(df_paragraphs, text_column)\n",
        "\n",
        "    # Filter out empty processed paragraphs\n",
        "    df_processed = df_processed[df_processed['processed_text'].str.len() > 0].copy()\n",
        "\n",
        "    print(f\"Original paragraphs: {len(df_paragraphs)}\")\n",
        "    print(f\"Paragraphs after processing: {len(df_processed)}\")\n",
        "\n",
        "    return df_processed\n",
        "\n",
        "def analyze_paragraphs_bow(df_processed, text_column='processed_text', top_n=20):\n",
        "    \"\"\"\n",
        "    Specific function to analyze preprocessed paragraphs with BOW\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BAG OF WORDS ANALYSIS - PROCESSED PARAGRAPHS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    top_words_df = create_bow_analysis(df_processed[text_column], top_n=top_n)\n",
        "\n",
        "    print(f\"Top {len(top_words_df)} most frequent lemmas:\")\n",
        "    print(top_words_df.to_string(index=False))\n",
        "\n",
        "    return top_words_df\n",
        "\n",
        "\n",
        "def run_paragraphs_analysis_pipeline(df_paragraphs, text_column='text', top_n=20):\n",
        "    \"\"\"\n",
        "    Complete pipeline for analyzing your paragraphs data\n",
        "    \"\"\"\n",
        "    # Step 1: Preprocess data\n",
        "    df_processed = process_paragraphs_dataframe(df_paragraphs, text_column)\n",
        "\n",
        "    # Step 2: Generate reports\n",
        "    stats = print_preprocessing_report(df_processed)\n",
        "    print_sample_transformations(df_processed)\n",
        "\n",
        "    # Step 3: BOW analysis\n",
        "    top_words = analyze_paragraphs_bow(df_processed, top_n=top_n)\n",
        "\n",
        "    return df_processed, top_words, stats\n",
        "\n",
        "# =============================================================================\n",
        "# USO\n",
        "# =============================================================================\n",
        "\n",
        "# Complete pipeline (your main use case)\n",
        "if 'df_consolidado' in locals() and not df_consolidado.empty:\n",
        "    print(f\"Analizando corpus de {df_consolidado['fecha_boletin'].nunique()} fechas...\\n\")\n",
        "\n",
        "    df_processed, top_words, stats = run_paragraphs_analysis_pipeline(df_consolidado)\n",
        "\n",
        "    # Opcional: Ver si el top 20 cambia mucho al tener más fechas\n",
        "    print(\"\\nTop palabras del periodo completo:\")\n",
        "    print(top_words)\n",
        "else:\n",
        "    print(\"No hay datos consolidados para analizar.\")\n",
        "\n",
        "# Just BOW analysis on already processed data\n",
        "#top_words = analyze_paragraphs_bow(df_processed)\n",
        "\n",
        "# Compare original vs processed BOW\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON: ORIGINAL vs PROCESSED BOW\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "bow_original = create_bow_analysis(df_processed['text'], top_n=30)\n",
        "bow_processed = create_bow_analysis(df_processed['processed_text'], top_n=30)\n",
        "\n",
        "print(\"Top 30 words - ORIGINAL:\")\n",
        "print(bow_original.to_string(index=False))\n",
        "\n",
        "print(\"\\nTop 30 words - PROCESSED (lemmatized):\")\n",
        "print(bow_processed.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5iwoqYToOEB",
        "outputId": "a3843fca-8a33-4833-e510-e85f27f1954a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analizando corpus de 22 fechas...\n",
            "\n",
            "Applying lemmatization to paragraphs...\n",
            "Original paragraphs: 2420\n",
            "Paragraphs after processing: 2420\n",
            "============================================================\n",
            "TEXT PREPROCESSING REPORT\n",
            "============================================================\n",
            "Total paragraphs: 2,420\n",
            "Total words - Original: 1,424,962\n",
            "Total words - Processed: 532,422\n",
            "Word reduction: 62.6%\n",
            "Unique words - Original: 7,683\n",
            "Unique words - Processed: 3,165\n",
            "Vocabulary reduction: 58.8%\n",
            "\n",
            "============================================================\n",
            "SAMPLE TRANSFORMATIONS\n",
            "============================================================\n",
            "\n",
            "Sample 1 (Page 1):\n",
            "ORIGINAL: Registro Nacional de la Propiedad Intelectual Nº 5.218.874\n",
            "DOMICILIO LEGAL: Suipacha 767 - C1008AAO\n",
            "Ciudad Autónoma de Buenos Aires\n",
            "Tel. 5218-8400Los ...\n",
            "PROCESSED: propiedad intelectual domicilio legal suipachaber ciudad autónomo tel documento aparecer república argentino tener auténtico obligatorio efecto public...\n",
            "Words: 117 → 49\n",
            "\n",
            "Sample 2 (Page 2):\n",
            "ORIGINAL: BOLETÍN OFICIAL Nº 35.809 - Primera Sección 2 Viernes 12 de diciembre de 2025\n",
            "SUMARIO\n",
            "Avisos Nuevos\n",
            "Decretos\n",
            "NOMENCLATURA COMÚN DEL MERCOSUR. Decreto ...\n",
            "PROCESSED: viernes diciembre sumario aviso decreto nomenclatura común mercosur derecho exportación instituto yerba mate designación jefatura gabinete ministro de...\n",
            "Words: 396 → 116\n",
            "\n",
            "Sample 3 (Page 3):\n",
            "ORIGINAL: BOLETÍN OFICIAL Nº 35.809 - Primera Sección 3 Viernes 12 de diciembre de 2025\n",
            "COMISIÓN ARBITRAL CONVENIO MULTILATERAL DEL 18.8.77. Resolución General ...\n",
            "PROCESSED: viernes diciembre comisión arbitral convenio multilateral general comisión arbitral convenio multilateral general comisión arbitral convenio multilate...\n",
            "Words: 201 → 82\n",
            "\n",
            "============================================================\n",
            "BAG OF WORDS ANALYSIS - PROCESSED PARAGRAPHS\n",
            "============================================================\n",
            "Top 20 most frequent lemmas:\n",
            "          word  frequency\n",
            "       trabajo       7062\n",
            "     diciembre       4466\n",
            "     seguridad       4290\n",
            "        empleo       3256\n",
            "     argentino       3102\n",
            " modificatoria       2992\n",
            "     colectivo       2728\n",
            "       general       2684\n",
            "      servicio       2684\n",
            "      relación       2618\n",
            "        social       2530\n",
            "       viernes       2508\n",
            "administrativo       2464\n",
            "    regulación       2442\n",
            "        humano       2376\n",
            "       capital       2332\n",
            "         cargo       2332\n",
            "            él       2332\n",
            "      convenio       2266\n",
            "administración       2178\n",
            "\n",
            "Top palabras del periodo completo:\n",
            "              word  frequency\n",
            "0          trabajo       7062\n",
            "1        diciembre       4466\n",
            "2        seguridad       4290\n",
            "3           empleo       3256\n",
            "4        argentino       3102\n",
            "5    modificatoria       2992\n",
            "6        colectivo       2728\n",
            "7          general       2684\n",
            "8         servicio       2684\n",
            "9         relación       2618\n",
            "10          social       2530\n",
            "11         viernes       2508\n",
            "12  administrativo       2464\n",
            "13      regulación       2442\n",
            "14          humano       2376\n",
            "15         capital       2332\n",
            "16           cargo       2332\n",
            "17              él       2332\n",
            "18        convenio       2266\n",
            "19  administración       2178\n",
            "\n",
            "============================================================\n",
            "COMPARISON: ORIGINAL vs PROCESSED BOW\n",
            "============================================================\n",
            "Top 30 words - ORIGINAL:\n",
            "      word  frequency\n",
            "        de     137522\n",
            "        la      68662\n",
            "         y      47388\n",
            "        el      40612\n",
            "       del      39930\n",
            "        en      31548\n",
            "       que      31108\n",
            "         a      27808\n",
            "         n      18854\n",
            "       los      17072\n",
            "       por      16214\n",
            "       las      15158\n",
            "  artículo      13354\n",
            "        se      12716\n",
            "  nacional      10934\n",
            "       apn       9790\n",
            "         o       8162\n",
            "      para       7502\n",
            "       ley       7392\n",
            "ministerio       7172\n",
            "   trabajo       7062\n",
            " dirección       7018\n",
            "       con       6864\n",
            "        su       6490\n",
            "        lo       6138\n",
            "       sus       6072\n",
            "  presente       5962\n",
            "resolución       5940\n",
            "   decreto       5676\n",
            "        al       5126\n",
            "\n",
            "Top 30 words - PROCESSED (lemmatized):\n",
            "          word  frequency\n",
            "       trabajo       7062\n",
            "     diciembre       4466\n",
            "     seguridad       4290\n",
            "        empleo       3256\n",
            "     argentino       3102\n",
            " modificatoria       2992\n",
            "     colectivo       2728\n",
            "       general       2684\n",
            "      servicio       2684\n",
            "      relación       2618\n",
            "        social       2530\n",
            "       viernes       2508\n",
            "administrativo       2464\n",
            "    regulación       2442\n",
            "        humano       2376\n",
            "       capital       2332\n",
            "         cargo       2332\n",
            "            él       2332\n",
            "      convenio       2266\n",
            "administración       2178\n",
            "      personal       1980\n",
            "        ciudad       1936\n",
            "       gestión       1936\n",
            "       sistema       1892\n",
            "  corresponder       1848\n",
            "    establecer       1848\n",
            "     república       1782\n",
            "       nuclear       1760\n",
            "      economía       1738\n",
            "      sindical       1738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 1. Generamos el dataframe con TODAS las palabras (o un número muy alto, ej: 10.000)\n",
        "# Usamos tu función existente 'create_bow_analysis' pero pedimos más palabras\n",
        "print(\"Generando reporte completo...\")\n",
        "df_frecuencias_full = create_bow_analysis(df_processed['processed_text'], top_n=10000)\n",
        "\n",
        "# 2. Definimos el nombre del archivo\n",
        "nombre_archivo = \"frecuencias_palabras_bora.csv\"\n",
        "\n",
        "# 3. Guardamos en CSV\n",
        "# Usamos encoding='utf-8-sig' para que Excel reconozca bien los acentos (ñ, á, é...)\n",
        "df_frecuencias_full.to_csv(nombre_archivo, index=False, encoding='utf-8-sig')\n",
        "\n",
        "# 4. Descargamos el archivo a tu PC\n",
        "files.download(nombre_archivo)\n",
        "\n",
        "print(f\"✓ Archivo '{nombre_archivo}' generado y descargado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ChLHsE31to5K",
        "outputId": "7fb1774e-4be7-4d14-9933-cb29eca99b54"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generando reporte completo...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_970af5e4-3bba-4262-9f67-da36b5034514\", \"frecuencias_palabras_bora.csv\", 40193)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Archivo 'frecuencias_palabras_bora.csv' generado y descargado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Otros\n",
        "\n",
        "Algunos scripts para explorar el contenido de los PDFs"
      ],
      "metadata": {
        "id": "P5QNMWw7HOE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tabula\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "import requests\n",
        "\n",
        "# Since we're in Colab, let's work with the downloaded PDF file\n",
        "pdf_path = \"boletin_pdfs/seccion_primera_20250927.pdf\"\n",
        "\n",
        "# First, let's get some basic information about the PDF\n",
        "# Get the number of pages\n",
        "import PyPDF2\n",
        "with open(pdf_path, 'rb') as file:\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    num_pages = len(pdf_reader.pages)\n",
        "    print(f\"Number of pages in the PDF: {num_pages}\")\n",
        "\n",
        "# Now let's extract all the text to search for \"derechos humanos\"\n",
        "full_text = \"\"\n",
        "for page_num in range(num_pages):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        full_text += page.extract_text()\n",
        "\n",
        "# Count occurrences of \"derechos humanos\"\n",
        "derechos_humanos_count = full_text.lower().count(\"derechos humanos\")\n",
        "print(f\"Occurrences of 'derechos humanos': {derechos_humanos_count}\")\n",
        "\n",
        "# Now let's use tabula to extract tables and look for resoluciones and disposiciones\n",
        "# Let's try to extract all tables from the PDF\n",
        "tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n",
        "print(f\"Number of tables found: {len(tables)}\")\n",
        "\n",
        "# Let's examine the structure of the first few tables to understand the content\n",
        "for i, table in enumerate(tables[:5]):\n",
        "    print(f\"\\nTable {i+1} shape: {table.shape}\")\n",
        "    print(f\"Table {i+1} columns: {table.columns.tolist()}\")\n",
        "    if not table.empty:\n",
        "        print(table.head(3))"
      ],
      "metadata": {
        "id": "LBnXb4Q8F8Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listar resoluciones\n",
        "\n",
        "# Extract text from page 2 specifically\n",
        "with open(pdf_path, 'rb') as file:\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    page2_text = pdf_reader.pages[1].extract_text()  # Page index 1 is page 2\n",
        "\n",
        "print(\"Content from page 2 (SUMARIO):\")\n",
        "print(page2_text[:2000])  # First 2000 characters\n",
        "\n",
        "# Let's search for resolution patterns in page 2 specifically\n",
        "resoluciones_page2 = re.findall(resoluciones_pattern, page2_text, re.IGNORECASE)\n",
        "print(f\"\\nResoluciones found in SUMARIO page: {len(set(resoluciones_page2))}\")\n",
        "for resol in sorted(set(resoluciones_page2)):\n",
        "    print(f\"  - {resol}\")\n",
        "\n",
        "# Let's also check if we can find the specific sections for Resoluciones and Disposiciones\n",
        "# by looking at the table of contents structure"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hndEDCh4G72j",
        "outputId": "bda3ea44-d13e-43ee-c972-7637f39938f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content from page 2 (SUMARIO):\n",
            " BOLETÍN OFICIAL Nº 35.774 - Primera Sección 2 Martes 21 de octubre de 2025\n",
            "SUMARIO\n",
            "Avisos Nuevos\n",
            "Leyes\n",
            "LEY DE FINANCIAMIENTO DE LA EDUCACIÓN UNIVERSITARIA Y RECOMPOSICIÓN DEL SALARIO DOCENTE. Ley 27795. Disposiciones.  ................  4\n",
            "Decreto 759/2025. DECTO-2025-759-APN-PTE - Promúlgase la Ley N° 27.795.  ........................................................................................................  6\n",
            "EMERGENCIA SANITARIA DE LA SALUD PEDIÁTRICA Y DE LAS RESIDENCIAS NACIONALES EN SALUD. Ley 27796. Disposiciones.  .......................  11\n",
            "Decreto 760/2025. DECTO-2025-760-APN-PTE - Promúlgase la Ley N° 27.796.  ........................................................................................................  12\n",
            "Resoluciones\n",
            "DIRECCIÓN NACIONAL DE VIALIDAD. Resolución 1619/2025. RESOL-2025-1619-APN-DNV#MEC.  ..........................................................................  17\n",
            "DIRECCIÓN NACIONAL DE VIALIDAD. Resolución 1620/2025. RESOL-2025-1620-APN-DNV#MEC.  ..........................................................................  20\n",
            "ENTE NACIONAL REGULADOR DEL GAS. Resolución 778/2025. RESOL-2025-778-APN-DIRECTORIO#ENARGAS.  .....................................................  24\n",
            "INSTITUTO NACIONAL DE CINE Y ARTES AUDIOVISUALES. Resolución 646/2025. RESOL-2025-646-APN-INCAA#SC.  ............................................  27\n",
            "JEFATURA DE GABINETE DE MINISTROS. Resolución 151/2025. RESOL-2025-151-APN-JGM.  ................................................................................  29\n",
            "JEFATURA DE GABINETE DE MINISTROS. SECRETARÍA DE INNOVACIÓN, CIENCIA Y TECNOLOGÍA. Resolución 264/2025. RESOL-2025-264-APN-\n",
            "SICYT#JGM.  .................................................................................................................................................................................................................  30\n",
            "MINISTERIO DE ECONOMÍA. SECRETARÍA DE ENERGÍA. Resolución 400/2025. RESOL-2025-400-APN-SE#MEC.  ...\n",
            "\n",
            "Resoluciones found in SUMARIO page: 21\n",
            "  - 108/2025\n",
            "  - 151/2025\n",
            "  - 1619/2025\n",
            "  - 1620/2025\n",
            "  - 1828/2025\n",
            "  - 1856/2025\n",
            "  - 243/2025\n",
            "  - 244/2025\n",
            "  - 245/2025\n",
            "  - 246/2025\n",
            "  - 247/2025\n",
            "  - 250/2025\n",
            "  - 264/2025\n",
            "  - 34/2025\n",
            "  - 35/2025\n",
            "  - 36/2025\n",
            "  - 400/2025\n",
            "  - 41/2025\n",
            "  - 520/2025\n",
            "  - 646/2025\n",
            "  - 778/2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe de prueba\n",
        "# Cambiar el pdf path al que se haya descargado en 📁\n",
        "pdf_path = \"boletin_pdfs/seccion_primera_20251020.pdf\"\n",
        "with open(pdf_path, 'rb') as file:\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    num_pages = len(pdf_reader.pages)\n",
        "    print(f\"Number of pages in the PDF: {num_pages}\")\n",
        "\n",
        "full_text = \"\"\n",
        "for page_num in range(num_pages):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        full_text += page.extract_text()\n",
        "df_test =  full_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mqam4ivskxt",
        "outputId": "7e021d23-c5b0-46ff-d834-26782c46bbdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages in the PDF: 124\n"
          ]
        }
      ]
    }
  ]
}