{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preparaci√≥n del entorno"
      ],
      "metadata": {
        "id": "CXOBm9wBGfa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "!pip install tabula-py\n",
        "!pip install pyPDF2\n",
        "# Import\n",
        "import tabula as tb\n",
        "import pandas as pd\n",
        "import requests\n",
        "import base64\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import os\n",
        "import PyPDF2\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmyqpkxRGJJU",
        "outputId": "41514f80-0fe4-4781-cda7-5933e3ee64cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.12/dist-packages (from tabula-py) (2.2.2)\n",
            "Requirement already satisfied: numpy>1.24.4 in /usr/local/lib/python3.12/dist-packages (from tabula-py) (2.0.2)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.12/dist-packages (from tabula-py) (1.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.17.0)\n",
            "Downloading tabula_py-2.10.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tabula-py\n",
            "Successfully installed tabula-py-2.10.0\n",
            "Collecting pyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyPDF2\n",
            "Successfully installed pyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V7dtDIyRD6EO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Descarga de PDFs de BORA\n",
        "BORA genera PDFs de los boletines de cada d√≠a.\n",
        "La primera secci√≥n es la que hace menci√≥n a Legislaci√≥n y avisos oficiales"
      ],
      "metadata": {
        "id": "xq2mPz5yGlIj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm3Qjzg1BpdU",
        "outputId": "5b6114ed-c73a-4274-b4fa-eaae05a91c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Downloaded: seccion_primera_20251021.pdf (2842359 bytes)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "class BoletinDownloader:\n",
        "    def __init__(self, download_folder=\"boletin_pdfs\"):\n",
        "        self.session = requests.Session()\n",
        "        self.base_url = \"https://www.boletinoficial.gob.ar\"\n",
        "        self.download_url = f\"{self.base_url}/pdf/download_section\"\n",
        "        self.download_folder = download_folder\n",
        "\n",
        "        # Create download folder if it doesn't exist\n",
        "        os.makedirs(download_folder, exist_ok=True)\n",
        "\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "            'X-Requested-With': 'XMLHttpRequest',\n",
        "            'Origin': self.base_url,\n",
        "            'Referer': f'{self.base_url}/'\n",
        "        }\n",
        "\n",
        "    def download_section(self, section, date):\n",
        "        \"\"\"Download a specific section for a given date\"\"\"\n",
        "        date_str = date.strftime(\"%Y%m%d\")\n",
        "        filename = f\"seccion_{section}_{date_str}.pdf\"\n",
        "        filepath = os.path.join(self.download_folder, filename)\n",
        "\n",
        "        # Check if file already exists\n",
        "        if os.path.exists(filepath):\n",
        "            print(f\"‚úì File already exists: {filename}\")\n",
        "            return True\n",
        "\n",
        "        data = {'nombreSeccion': section}\n",
        "\n",
        "        try:\n",
        "            response = self.session.post(\n",
        "                self.download_url,\n",
        "                data=data,\n",
        "                headers=self.headers,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                json_data = response.json()\n",
        "\n",
        "                if 'pdfBase64' in json_data and json_data['pdfBase64']:\n",
        "                    pdf_data = base64.b64decode(json_data['pdfBase64'])\n",
        "\n",
        "                    with open(filepath, 'wb') as f:\n",
        "                        f.write(pdf_data)\n",
        "\n",
        "                    print(f\"‚úì Downloaded: {filename} ({len(pdf_data)} bytes)\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(f\"‚úó No PDF available for {date_str}\")\n",
        "                    return False\n",
        "            else:\n",
        "                print(f\"‚úó HTTP {response.status_code} for {date_str}\")\n",
        "                return False\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"‚úó Network error for {date_str}: {e}\")\n",
        "            return False\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚úó JSON decode error for {date_str}: {e}\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"‚úó Unexpected error for {date_str}: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "# test with today's date\n",
        "downloader = BoletinDownloader()\n",
        "test_date = datetime.now()\n",
        "downloader.download_section(\"primera\", test_date)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Implementando Bag of words\n",
        "Bag of words es un modelo muy utilizado en text mining.\n",
        "Tiene la particularidad de adaptarse bien a corpus textuales muy diversos permitiendo describir el corpus sin ser especialista en el dominio"
      ],
      "metadata": {
        "id": "uCOtpYDlHSQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Preparar dataset e importarinstalar librerias"
      ],
      "metadata": {
        "id": "onLZ9lYl-3mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import and install\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "spanish_stopwords = stopwords.words('spanish')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import spacy\n",
        "import locale\n",
        "import string\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "! spacy download es_core_news_sm\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n"
      ],
      "metadata": {
        "id": "ob7G_Q0sHmlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e538dec1-596d-4bd2-9fd3-52e65014043c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe organized in paragraphs\n",
        "def extract_paragraphs_to_dataframe(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text at paragraph level for better context\n",
        "    \"\"\"\n",
        "    paragraphs_data = []\n",
        "\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "\n",
        "            # Split into paragraphs\n",
        "            page_paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "            for para_num, paragraph in enumerate(page_paragraphs):\n",
        "                paragraphs_data.append({\n",
        "                    'page': page_num + 1,\n",
        "                    'paragraph_id': f\"p{page_num+1}_{para_num+1}\",\n",
        "                    'text': paragraph,\n",
        "                    'char_count': len(paragraph),\n",
        "                    'word_count': len(paragraph.split())\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(paragraphs_data)\n",
        "\n",
        "# Usage\n",
        "pdf_path = \"boletin_pdfs/seccion_primera_20251021.pdf\"\n",
        "df_paragraphs = extract_paragraphs_to_dataframe(pdf_path)\n",
        "print(df_paragraphs.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgLAwDugv6FH",
        "outputId": "7d16079b-f6ed-4fdc-f8e3-449cfee49caf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   page paragraph_id                                               text  \\\n",
            "0     1         p1_1  Registro Nacional de la Propiedad Intelectual ...   \n",
            "1     2         p2_1  BOLET√çN OFICIAL N¬∫ 35.774 - Primera Secci√≥n 2 ...   \n",
            "2     3         p3_1  BOLET√çN OFICIAL N¬∫ 35.774 - Primera Secci√≥n 3 ...   \n",
            "3     4         p4_1  BOLET√çN OFICIAL N¬∫ 35.774 - Primera Secci√≥n 4 ...   \n",
            "4     5         p5_1  BOLET√çN OFICIAL N¬∫ 35.774 - Primera Secci√≥n 5 ...   \n",
            "\n",
            "   char_count  word_count  \n",
            "0         827         117  \n",
            "1        5789         392  \n",
            "2        2239          82  \n",
            "3        3367         508  \n",
            "4        5532         812  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Funciones para BOW"
      ],
      "metadata": {
        "id": "s3kFrcVK-uTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_spanish_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess a single Spanish text with lemmatization\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text == '':\n",
        "        return \"\"\n",
        "\n",
        "    doc = nlp(text.lower())\n",
        "\n",
        "    lemmas = []\n",
        "    for token in doc:\n",
        "        if (not token.is_stop and\n",
        "            not token.is_punct and\n",
        "            not token.is_space and\n",
        "            len(token.text) > 2 and\n",
        "            token.is_alpha):\n",
        "            lemmas.append(token.lemma_)\n",
        "\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "def apply_text_preprocessing(df, text_column):\n",
        "    \"\"\"\n",
        "    Apply preprocessing to a DataFrame column\n",
        "    Returns a new DataFrame with processed text\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    df_processed['processed_text'] = df_processed[text_column].apply(preprocess_spanish_text)\n",
        "    df_processed['original_word_count'] = df_processed[text_column].str.split().str.len()\n",
        "    df_processed['processed_word_count'] = df_processed['processed_text'].str.split().str.len()\n",
        "\n",
        "    return df_processed\n",
        "\n",
        "def calculate_text_statistics(df, original_col, processed_col):\n",
        "    \"\"\"\n",
        "    Calculate text statistics for comparison\n",
        "    \"\"\"\n",
        "    total_original = df[original_col].str.split().str.len().sum()\n",
        "    total_processed = df[processed_col].str.split().str.len().sum()\n",
        "\n",
        "    all_original = ' '.join(df[original_col].astype(str))\n",
        "    all_processed = ' '.join(df[processed_col].astype(str))\n",
        "\n",
        "    unique_original = len(set(all_original.lower().split()))\n",
        "    unique_processed = len(set(all_processed.split()))\n",
        "\n",
        "    stats = {\n",
        "        'total_paragraphs': len(df),\n",
        "        'total_original_words': total_original,\n",
        "        'total_processed_words': total_processed,\n",
        "        'unique_original_words': unique_original,\n",
        "        'unique_processed_words': unique_processed,\n",
        "        'word_reduction_pct': ((total_original - total_processed) / total_original * 100),\n",
        "        'vocab_reduction_pct': ((unique_original - unique_processed) / unique_original * 100)\n",
        "    }\n",
        "\n",
        "    return stats\n",
        "\n",
        "def create_bow_analysis(text_series, top_n=20, **vectorizer_kwargs):\n",
        "    \"\"\"\n",
        "    Create Bag-of-Words analysis from a text series\n",
        "    \"\"\"\n",
        "    # Combine all text\n",
        "    all_text = ' '.join(text_series.astype(str))\n",
        "\n",
        "    # Default vectorizer parameters\n",
        "    default_params = {\n",
        "        'max_features': top_n * 2,\n",
        "        'lowercase': True,\n",
        "        'token_pattern': r'\\b[a-zA-Z√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë]+\\b'\n",
        "    }\n",
        "    default_params.update(vectorizer_kwargs)\n",
        "\n",
        "    vectorizer = CountVectorizer(**default_params)\n",
        "    bow_matrix = vectorizer.fit_transform([all_text])\n",
        "\n",
        "    # Get word frequencies\n",
        "    word_freq = dict(zip(vectorizer.get_feature_names_out(), bow_matrix.sum(axis=0).tolist()[0]))\n",
        "    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "    return pd.DataFrame(top_words, columns=['word', 'frequency'])\n",
        "\n",
        "\n",
        "def print_preprocessing_report(df, original_col='text', processed_col='processed_text'):\n",
        "    \"\"\"\n",
        "    Print a comprehensive preprocessing report\n",
        "    \"\"\"\n",
        "    stats = calculate_text_statistics(df, original_col, processed_col)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"TEXT PREPROCESSING REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"Total paragraphs: {stats['total_paragraphs']:,}\")\n",
        "    print(f\"Total words - Original: {stats['total_original_words']:,}\")\n",
        "    print(f\"Total words - Processed: {stats['total_processed_words']:,}\")\n",
        "    print(f\"Word reduction: {stats['word_reduction_pct']:.1f}%\")\n",
        "    print(f\"Unique words - Original: {stats['unique_original_words']:,}\")\n",
        "    print(f\"Unique words - Processed: {stats['unique_processed_words']:,}\")\n",
        "    print(f\"Vocabulary reduction: {stats['vocab_reduction_pct']:.1f}%\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "def print_sample_transformations(df, original_col='text', processed_col='processed_text', sample_size=3):\n",
        "    \"\"\"\n",
        "    Print sample transformations for quality check\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SAMPLE TRANSFORMATIONS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for i in range(min(sample_size, len(df))):\n",
        "        original = df.iloc[i][original_col]\n",
        "        processed = df.iloc[i][processed_col]\n",
        "\n",
        "        # Try to get page number if available\n",
        "        page_info = f\"Page {df.iloc[i]['page']}\" if 'page' in df.columns else f\"Row {i+1}\"\n",
        "\n",
        "        print(f\"\\nSample {i+1} ({page_info}):\")\n",
        "        print(f\"ORIGINAL: {original[:150]}...\")\n",
        "        print(f\"PROCESSED: {processed[:150]}...\")\n",
        "\n",
        "        if 'original_word_count' in df.columns and 'processed_word_count' in df.columns:\n",
        "            print(f\"Words: {df.iloc[i]['original_word_count']} ‚Üí {df.iloc[i]['processed_word_count']}\")\n",
        "\n",
        "\n",
        "def process_paragraphs_dataframe(df_paragraphs, text_column='text'):\n",
        "    \"\"\"\n",
        "    Specific function to process your paragraphs DataFrame\n",
        "    \"\"\"\n",
        "    print(\"Applying lemmatization to paragraphs...\")\n",
        "    df_processed = apply_text_preprocessing(df_paragraphs, text_column)\n",
        "\n",
        "    # Filter out empty processed paragraphs\n",
        "    df_processed = df_processed[df_processed['processed_text'].str.len() > 0].copy()\n",
        "\n",
        "    print(f\"Original paragraphs: {len(df_paragraphs)}\")\n",
        "    print(f\"Paragraphs after processing: {len(df_processed)}\")\n",
        "\n",
        "    return df_processed\n",
        "\n",
        "def analyze_paragraphs_bow(df_processed, text_column='processed_text', top_n=20):\n",
        "    \"\"\"\n",
        "    Specific function to analyze preprocessed paragraphs with BOW\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BAG OF WORDS ANALYSIS - PROCESSED PARAGRAPHS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    top_words_df = create_bow_analysis(df_processed[text_column], top_n=top_n)\n",
        "\n",
        "    print(f\"Top {len(top_words_df)} most frequent lemmas:\")\n",
        "    print(top_words_df.to_string(index=False))\n",
        "\n",
        "    return top_words_df\n",
        "\n",
        "\n",
        "def run_paragraphs_analysis_pipeline(df_paragraphs, text_column='text', top_n=20):\n",
        "    \"\"\"\n",
        "    Complete pipeline for analyzing your paragraphs data\n",
        "    \"\"\"\n",
        "    # Step 1: Preprocess data\n",
        "    df_processed = process_paragraphs_dataframe(df_paragraphs, text_column)\n",
        "\n",
        "    # Step 2: Generate reports\n",
        "    stats = print_preprocessing_report(df_processed)\n",
        "    print_sample_transformations(df_processed)\n",
        "\n",
        "    # Step 3: BOW analysis\n",
        "    top_words = analyze_paragraphs_bow(df_processed, top_n=top_n)\n",
        "\n",
        "    return df_processed, top_words, stats\n",
        "\n",
        "# =============================================================================\n",
        "# USAGE\n",
        "# =============================================================================\n",
        "\n",
        "# Complete pipeline (your main use case)\n",
        "df_processed, top_words, stats = run_paragraphs_analysis_pipeline(df_paragraphs)\n",
        "\n",
        "# Just preprocessing\n",
        "# df_processed = process_paragraphs_dataframe(df_paragraphs)\n",
        "\n",
        "# Just BOW analysis on already processed data\n",
        "#top_words = analyze_paragraphs_bow(df_processed)\n",
        "\n",
        "# Compare original vs processed BOW\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON: ORIGINAL vs PROCESSED BOW\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "bow_original = create_bow_analysis(df_processed['text'], top_n=10)\n",
        "bow_processed = create_bow_analysis(df_processed['processed_text'], top_n=10)\n",
        "\n",
        "print(\"Top 10 words - ORIGINAL:\")\n",
        "print(bow_original.to_string(index=False))\n",
        "\n",
        "print(\"\\nTop 10 words - PROCESSED (lemmatized):\")\n",
        "print(bow_processed.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5iwoqYToOEB",
        "outputId": "48953011-e140-4589-cf6a-537402f92f6f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying lemmatization to paragraphs...\n",
            "Original paragraphs: 99\n",
            "Paragraphs after processing: 99\n",
            "============================================================\n",
            "TEXT PREPROCESSING REPORT\n",
            "============================================================\n",
            "Total paragraphs: 99\n",
            "Total words - Original: 61,072\n",
            "Total words - Processed: 27,114\n",
            "Word reduction: 55.6%\n",
            "Unique words - Original: 7,069\n",
            "Unique words - Processed: 2,979\n",
            "Vocabulary reduction: 57.9%\n",
            "\n",
            "============================================================\n",
            "SAMPLE TRANSFORMATIONS\n",
            "============================================================\n",
            "\n",
            "Sample 1 (Page 1):\n",
            "ORIGINAL: Registro Nacional de la Propiedad Intelectual N¬∫¬†5.218.874\n",
            "DOMICILIO LEGAL: Suipacha 767 - C1008AAO\n",
            "Ciudad Aut√≥noma de Buenos Aires\n",
            "Tel. 5218-8400Los ...\n",
            "PROCESSED: registro nacional propiedad intelectual domicilio legal suipachaber ciudad aut√≥nomo aires tel documento aparecer bolet√≠n oficial rep√∫blica argentino t...\n",
            "Words: 117 ‚Üí 66\n",
            "\n",
            "Sample 2 (Page 2):\n",
            "ORIGINAL: BOLET√çN OFICIAL N¬∫ 35.774 - Primera Secci√≥n 2 Martes 21 de octubre de 2025\n",
            "SUMARIO\n",
            "Avisos Nuevos\n",
            "Leyes\n",
            "LEY DE FINANCIAMIENTO DE LA EDUCACI√ìN UNIVERSIT...\n",
            "PROCESSED: bolet√≠n oficial secci√≥n martes octubre sumario aviso ley ley financiamiento educaci√≥n universitario recomposici√≥n salario docente ley disposici√≥n decr...\n",
            "Words: 392 ‚Üí 186\n",
            "\n",
            "Sample 3 (Page 3):\n",
            "ORIGINAL: BOLET√çN OFICIAL N¬∫ 35.774 - Primera Secci√≥n 3 Martes 21 de octubre de 2025\n",
            "Disposiciones\n",
            "AGENCIA DE RECAUDACI√ìN Y CONTROL ADUANERO. DIRECCI√ìN REGIONAL...\n",
            "PROCESSED: bolet√≠n oficial secci√≥n martes octubre disposici√≥n agencia recaudaci√≥n control aduanero direcci√≥n regional bah√≠a blanco disposici√≥n ministerio salud d...\n",
            "Words: 82 ‚Üí 41\n",
            "\n",
            "============================================================\n",
            "BAG OF WORDS ANALYSIS - PROCESSED PARAGRAPHS\n",
            "============================================================\n",
            "Top 20 most frequent lemmas:\n",
            "       word  frequency\n",
            "   art√≠culo        605\n",
            "   nacional        507\n",
            "        ley        458\n",
            "    trabajo        340\n",
            "  direcci√≥n        288\n",
            "   presente        262\n",
            " resoluci√≥n        240\n",
            "    decreto        212\n",
            "    oficial        184\n",
            "      fecha        173\n",
            "establecido        158\n",
            "disposici√≥n        152\n",
            " regulaci√≥n        150\n",
            " expediente        149\n",
            " ministerio        146\n",
            "    octubre        145\n",
            "   registro        144\n",
            "   conforme        139\n",
            "    p√∫blico        126\n",
            "   relaci√≥n        125\n",
            "\n",
            "============================================================\n",
            "COMPARISON: ORIGINAL vs PROCESSED BOW\n",
            "============================================================\n",
            "Top 10 words - ORIGINAL:\n",
            "word  frequency\n",
            "  de       5703\n",
            "  la       3173\n",
            "   y       2045\n",
            "  el       1920\n",
            " del       1667\n",
            "  en       1461\n",
            " que       1372\n",
            "   a       1151\n",
            "   n        879\n",
            " por        789\n",
            "\n",
            "Top 10 words - PROCESSED (lemmatized):\n",
            "      word  frequency\n",
            "  art√≠culo        605\n",
            "  nacional        507\n",
            "       ley        458\n",
            "   trabajo        340\n",
            " direcci√≥n        288\n",
            "  presente        262\n",
            "resoluci√≥n        240\n",
            "   decreto        212\n",
            "   oficial        184\n",
            "     fecha        173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Otros\n",
        "\n",
        "Algunos scripts para explorar el contenido de los PDFs"
      ],
      "metadata": {
        "id": "P5QNMWw7HOE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tabula\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "import requests\n",
        "\n",
        "# Since we're in Colab, let's work with the downloaded PDF file\n",
        "pdf_path = \"boletin_pdfs/seccion_primera_20250927.pdf\"\n",
        "\n",
        "# First, let's get some basic information about the PDF\n",
        "# Get the number of pages\n",
        "import PyPDF2\n",
        "with open(pdf_path, 'rb') as file:\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    num_pages = len(pdf_reader.pages)\n",
        "    print(f\"Number of pages in the PDF: {num_pages}\")\n",
        "\n",
        "# Now let's extract all the text to search for \"derechos humanos\"\n",
        "full_text = \"\"\n",
        "for page_num in range(num_pages):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        full_text += page.extract_text()\n",
        "\n",
        "# Count occurrences of \"derechos humanos\"\n",
        "derechos_humanos_count = full_text.lower().count(\"derechos humanos\")\n",
        "print(f\"Occurrences of 'derechos humanos': {derechos_humanos_count}\")\n",
        "\n",
        "# Now let's use tabula to extract tables and look for resoluciones and disposiciones\n",
        "# Let's try to extract all tables from the PDF\n",
        "tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n",
        "print(f\"Number of tables found: {len(tables)}\")\n",
        "\n",
        "# Let's examine the structure of the first few tables to understand the content\n",
        "for i, table in enumerate(tables[:5]):\n",
        "    print(f\"\\nTable {i+1} shape: {table.shape}\")\n",
        "    print(f\"Table {i+1} columns: {table.columns.tolist()}\")\n",
        "    if not table.empty:\n",
        "        print(table.head(3))"
      ],
      "metadata": {
        "id": "LBnXb4Q8F8Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listar resoluciones\n",
        "\n",
        "# Extract text from page 2 specifically\n",
        "with open(pdf_path, 'rb') as file:\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    page2_text = pdf_reader.pages[1].extract_text()  # Page index 1 is page 2\n",
        "\n",
        "print(\"Content from page 2 (SUMARIO):\")\n",
        "print(page2_text[:2000])  # First 2000 characters\n",
        "\n",
        "# Let's search for resolution patterns in page 2 specifically\n",
        "resoluciones_page2 = re.findall(resoluciones_pattern, page2_text, re.IGNORECASE)\n",
        "print(f\"\\nResoluciones found in SUMARIO page: {len(set(resoluciones_page2))}\")\n",
        "for resol in sorted(set(resoluciones_page2)):\n",
        "    print(f\"  - {resol}\")\n",
        "\n",
        "# Let's also check if we can find the specific sections for Resoluciones and Disposiciones\n",
        "# by looking at the table of contents structure"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hndEDCh4G72j",
        "outputId": "bda3ea44-d13e-43ee-c972-7637f39938f0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content from page 2 (SUMARIO):\n",
            " BOLET√çN OFICIAL N¬∫ 35.774 - Primera Secci√≥n 2 Martes 21 de octubre de 2025\n",
            "SUMARIO\n",
            "Avisos Nuevos\n",
            "Leyes\n",
            "LEY DE FINANCIAMIENTO DE LA EDUCACI√ìN UNIVERSITARIA Y RECOMPOSICI√ìN DEL SALARIO DOCENTE. Ley 27795. Disposiciones.  ................  4\n",
            "Decreto 759/2025. DECTO-2025-759-APN-PTE - Prom√∫lgase la Ley N¬∞¬†27.795.  ........................................................................................................  6\n",
            "EMERGENCIA SANITARIA DE LA SALUD PEDI√ÅTRICA Y DE LAS RESIDENCIAS NACIONALES EN SALUD. Ley 27796. Disposiciones.  .......................  11\n",
            "Decreto 760/2025. DECTO-2025-760-APN-PTE - Prom√∫lgase la Ley N¬∞¬†27.796.  ........................................................................................................  12\n",
            "Resoluciones\n",
            "DIRECCI√ìN NACIONAL DE VIALIDAD. Resoluci√≥n 1619/2025. RESOL-2025-1619-APN-DNV#MEC.  ..........................................................................  17\n",
            "DIRECCI√ìN NACIONAL DE VIALIDAD. Resoluci√≥n 1620/2025. RESOL-2025-1620-APN-DNV#MEC.  ..........................................................................  20\n",
            "ENTE NACIONAL REGULADOR DEL GAS. Resoluci√≥n 778/2025. RESOL-2025-778-APN-DIRECTORIO#ENARGAS.  .....................................................  24\n",
            "INSTITUTO NACIONAL DE CINE Y ARTES AUDIOVISUALES. Resoluci√≥n 646/2025. RESOL-2025-646-APN-INCAA#SC.  ............................................  27\n",
            "JEFATURA DE GABINETE DE MINISTROS. Resoluci√≥n 151/2025. RESOL-2025-151-APN-JGM.  ................................................................................  29\n",
            "JEFATURA DE GABINETE DE MINISTROS. SECRETAR√çA DE INNOVACI√ìN, CIENCIA Y TECNOLOG√çA. Resoluci√≥n 264/2025. RESOL-2025-264-APN-\n",
            "SICYT#JGM.  .................................................................................................................................................................................................................  30\n",
            "MINISTERIO DE ECONOM√çA. SECRETAR√çA DE ENERG√çA. Resoluci√≥n 400/2025. RESOL-2025-400-APN-SE#MEC.  ...\n",
            "\n",
            "Resoluciones found in SUMARIO page: 21\n",
            "  - 108/2025\n",
            "  - 151/2025\n",
            "  - 1619/2025\n",
            "  - 1620/2025\n",
            "  - 1828/2025\n",
            "  - 1856/2025\n",
            "  - 243/2025\n",
            "  - 244/2025\n",
            "  - 245/2025\n",
            "  - 246/2025\n",
            "  - 247/2025\n",
            "  - 250/2025\n",
            "  - 264/2025\n",
            "  - 34/2025\n",
            "  - 35/2025\n",
            "  - 36/2025\n",
            "  - 400/2025\n",
            "  - 41/2025\n",
            "  - 520/2025\n",
            "  - 646/2025\n",
            "  - 778/2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe de prueba\n",
        "# Cambiar el pdf path al que se haya descargado en üìÅ\n",
        "pdf_path = \"boletin_pdfs/seccion_primera_20251020.pdf\"\n",
        "with open(pdf_path, 'rb') as file:\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "    num_pages = len(pdf_reader.pages)\n",
        "    print(f\"Number of pages in the PDF: {num_pages}\")\n",
        "\n",
        "full_text = \"\"\n",
        "for page_num in range(num_pages):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        full_text += page.extract_text()\n",
        "df_test =  full_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mqam4ivskxt",
        "outputId": "7e021d23-c5b0-46ff-d834-26782c46bbdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages in the PDF: 124\n"
          ]
        }
      ]
    }
  ]
}